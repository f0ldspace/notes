---
id: 2025-01-01-Amanda Askell
aliases: []
tags:
  - people/ai
  - ai
  - anthropic
  - claude
  - philosophy
  - ai-safety
date: 2025-01-01 17:00:00 -0000
layout: post
---

## Overview

Amanda Askell is a philosopher and AI safety researcher who currently works at [[Anthropic]], where she leads efforts in AI alignment and model finetuning. Her work bridges philosophical inquiry with practical AI development, focusing particularly on creating AI systems that align with human values. She has earned recognition as one of the most influential people in AI, nicknamed the "Claude whisperer" for her pivotal role in shaping Claude's personality and ethical framework.

## Education and Academic Background

- **PhD in Philosophy** (2011-2018): New York University
- **BPhil in Philosophy** (2009-2011): University of Oxford
- **Undergraduate Degree in Philosophy**: University of Dundee

## Professional Career

- **Research Scientist (Alignment Finetuning)** (2021-Present): Anthropic
  - Leads a team focused on training AI models to be more honest and develop beneficial character traits
  - Works on developing new finetuning techniques to ensure interventions scale to more capable models
  - Plays a key role in shaping Claude's personality and ethical framework
- **Research Scientist (Policy)** (2018-2021): OpenAI
  - Focused on AI safety via debate and human baselines for AI performance
  - Worked on responsible AI development, cooperation, and policy

## Research Focus and Contributions

### Philosophy

Her philosophical work has centered around:

- Ethics, particularly infinite ethics and ethical frameworks for evaluating worlds with infinite populations
- [[Decision theory]]
- Formal epistemology
- [[Moral uncertainty]]

### AI Safety and Alignment

At Anthropic, Askell has focused on:

- Training models to be more honest and develop beneficial character traits
- Creating AI systems that are helpful, harmless, and honest
- Developing Claude's personality to be friendly, curious, and creative while avoiding potentially harmful traits
- Exploring ways to make AI systems admit uncertainty and discuss ideas without bias

## Notable Publications

1. "A General Language Assistant as a Laboratory for Alignment" (2021)
   - Explores how large language models can be aligned with human values
   - Investigates scaling trends for alignment training objectives
   - Compares effectiveness of different training approaches like imitation learning and ranked preference modeling
2. "Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training" (2024)
   - Examines how deceptive behaviors can be implanted in language models
   - Tests the robustness of safety training techniques in removing potentially harmful behaviors
   - Demonstrates that certain backdoor behaviors can persist despite safety training interventions
3. "Release Strategies and the Social Impacts of Language Models" (2019)
   - Discusses OpenAI's staged release approach for the GPT-2 language model
   - Examines risk and benefit analyses as model capabilities increase
   - Provides recommendations for responsible publication in AI

## Influence and Recognition

In 2024, Askell was named one of TIME magazine's 100 Most Influential People in AI. She's known as the "Claude whisperer" at Anthropic for her central role in developing Claude's personality. TIME notes her focus on creating AI that "openly admit[s] to users when it's unsure of its answer, [attempts] to discuss ideas without bias, and [avoids] both-sidesism when discussing settled issues like climate change."

## Philosophy on AI Development

Askell believes in creating AI systems that:

- Acknowledge their limitations and uncertainty
- Avoid presenting themselves as authoritative sources on all topics
- Display nuanced conceptions of what it means to be "good"
- Balance human-like interaction with avoiding risks of over-anthropomorphization
  In her work at Anthropic, she has helped shape Claude to signal "that you're talking with something that isn't this grand source of authority on everything" to discourage users from accepting outputs at face value without critical thinking.

## Personal Values

Askell is a member of Giving What We Can and has pledged to donate at least 10% of her lifetime income to charity, though she hopes to donate more than 50% if possible. This reflects her commitment to effective altruism principles.

## Speaking and Public Engagement

Askell has participated in several speaking engagements, including:

- NeurIPS Panel: "How Should a Machine Learning Researcher Think About AI Ethics?" (2021)
- Code.org Ethics of AI Video Series
- Various interviews and discussions on AI safety, ethics, and development

## Impact on AI Safety

Through her philosophical training and AI research experience, Askell represents a bridge between abstract ethical considerations and practical AI development. Her work has been particularly influential in:

- Developing frameworks for aligning AI systems with human values
- Creating methodologies for finetuning models to improve safety characteristics
- Establishing approaches for evaluating AI performance on ethical dimensions
- Exploring the persistence of potentially harmful behaviors in AI systems despite safety interventions

## Role at Anthropic

At Anthropic, Askell's work has been instrumental in developing Claude, particularly in:

- Creating a model with a thoughtfully engineered persona that balances helpfulness with appropriate limitations
- Training models that can openly acknowledge uncertainty
- Developing AI that can discuss contentious topics without falling into false equivalence
- Implementing safeguards against harmful or deceptive behaviors
  Her interdisciplinary background in philosophy and AI research has positioned her uniquely to address the complex ethical challenges that arise in the development of increasingly capable AI systems.
